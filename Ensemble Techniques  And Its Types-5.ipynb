{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ca4dbd-8ba4-4f47-8510-cbaeb244c5f1",
   "metadata": {},
   "source": [
    "Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and\n",
    "categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are\n",
    "m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
    "eng#neer#ng process and handles the m#ss#ng valuesD\n",
    "Des#gn a p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Use an automated feature select#on method to #dent#fy the #mportant features #n the datasetC\n",
    "Create a numer#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Impute the m#ss#ng values #n the numer#cal columns us#ng the mean of the column valuesC\n",
    "Scale the numer#cal columns us#ng standard#sat#onC\n",
    "Create a categor#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Impute the m#ss#ng values #n the categor#cal columns us#ng the most frequent value of the columnC\n",
    "One-hot encode the categor#cal columnsC\n",
    "Comb#ne the numer#cal and categor#cal p#pel#nes us#ng a ColumnTransformerC\n",
    "Use a Random Forest Class#f#er to bu#ld the f#nal modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "Note! Your solut#on should #nclude code sn#ppets for each step of the p#pel#ne, and a br#ef explanat#on of\n",
    "each step. You should also prov#de an #nterpretat#on of the results and suggest poss#ble #mprovements for\n",
    "the p#pel#neD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6590424e-4622-435a-ab9c-512b7d75184d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming 'X' is your feature matrix and 'y' is the target variable\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Numerical pipeline\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#numerical_features = # List of numerical feature names\u001b[39;00m\n\u001b[1;32m     15\u001b[0m numerical_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     16\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     17\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler())\n\u001b[1;32m     18\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numerical pipeline\n",
    "#numerical_features = # List of numerical feature names\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "#categorical_features = List of categorical feature names\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Feature selection using Random Forest\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Combined pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', feature_selector),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on the test dataset: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1a74e-a602-4e7a-8fb6-f679d1f93aa6",
   "metadata": {},
   "source": [
    "Numerical Pipeline:\n",
    "\n",
    "Impute missing values in numerical columns using the mean of the column values.\n",
    "Scale numerical columns using standardization.\n",
    "Categorical Pipeline:\n",
    "\n",
    "Impute missing values in categorical columns using the most frequent value of the column.\n",
    "One-hot encode categorical columns.\n",
    "Feature Selection:\n",
    "\n",
    "Use a Random Forest classifier for feature selection to identify important features.\n",
    "Combined Pipeline:\n",
    "\n",
    "Use ColumnTransformer to combine the numerical and categorical pipelines.\n",
    "The combined pipeline includes preprocessing steps for both numerical and categorical features.\n",
    "Feature selection is applied after preprocessing.\n",
    "Random Forest Classifier:\n",
    "\n",
    "Use a Random Forest classifier as the final model.\n",
    "Evaluation:\n",
    "\n",
    "Make predictions on the test dataset and evaluate accuracy.\n",
    "Possible Improvements:\n",
    "\n",
    "Fine-tune hyperparameters of the Random Forest Classifier for better performance.\n",
    "Experiment with different feature selection methods.\n",
    "Consider handling imbalanced classes if present in the target variable.\n",
    "Explore additional preprocessing steps or feature engineering techniques based on domain knowledge.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55ec94-2d7e-4ed1-b71a-eea845164f9f",
   "metadata": {},
   "source": [
    "Q2. Build a pipel#ne that #ncludes a random forest class#f#er and a log#st#c regress#on class#f#er, and then\n",
    "use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb4f198-d84a-4944-8872-53c42644c352",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming 'X' is your feature matrix and 'y' is the target variable\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create pipelines for Random Forest and Logistic Regression\u001b[39;00m\n\u001b[1;32m     13\u001b[0m rf_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     15\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[1;32m     16\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m     17\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipelines for Random Forest and Logistic Regression\n",
    "rf_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Create a Voting Classifier combining Random Forest and Logistic Regression\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_pipeline),\n",
    "        ('lr', lr_pipeline)\n",
    "    ],\n",
    "    voting='hard'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Fit the Voting Classifier on training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy using Voting Classifier: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204727e-982f-43be-b6f1-f38ecbe77dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
